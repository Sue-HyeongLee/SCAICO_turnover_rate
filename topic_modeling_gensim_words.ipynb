{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Gensim\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bow_df.csv')\n",
    "# 결측치 제거\n",
    "df = df.dropna()\n",
    "\n",
    "company_name = df.company_name\n",
    "company_name.to_csv(\"company_name.csv\")\n",
    "other_var = df.drop(['company_name','adv','dadv','Unnamed: 0'],axis=1)\n",
    "other_var.to_csv(\"other_var.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "t = Okt()\n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    return [\n",
    "        token\n",
    "        for token, pos in t.pos(doc)\n",
    "        if pos in ['Noun','Verb','Adjective'] and len(token)>1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_adv = [my_tokenizer(text) for text in df.adv]\n",
    "text_dadv = [my_tokenizer(text) for text in df.dadv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Number of initial unique words in adv_documents: 44973\n",
      "#Number of initial unique words in dadv_documents: 75796\n",
      "#Number of unique words after removing rae and common words: 2000\n",
      "#Number of unique words after removing rae and common words: 2000\n",
      "#Number of unique tokens: 2000\n",
      "#Number of documents: 2503\n",
      "#Number of unique tokens: 2000\n",
      "#Number of documents: 2503\n"
     ]
    }
   ],
   "source": [
    "dictionary_adv = Dictionary(text_adv)\n",
    "dictionary_dadv = Dictionary(text_dadv)\n",
    "print('#Number of initial unique words in adv_documents:',len(dictionary_adv))\n",
    "print('#Number of initial unique words in dadv_documents:',len(dictionary_dadv))\n",
    "\n",
    "dictionary_adv.filter_extremes(keep_n = 2000, no_below = 10, no_above = 0.5)\n",
    "dictionary_dadv.filter_extremes(keep_n = 2000, no_below = 10, no_above = 0.5)\n",
    "\n",
    "print(\"#Number of unique words after removing rae and common words:\", len(dictionary_adv))\n",
    "print(\"#Number of unique words after removing rae and common words:\", len(dictionary_dadv))\n",
    "\n",
    "corpus_adv = [dictionary_adv.doc2bow(text) for text in text_adv]\n",
    "corpus_dadv = [dictionary_dadv.doc2bow(text) for text in text_dadv]\n",
    "print('#Number of unique tokens: %d' % len(dictionary_adv))\n",
    "print('#Number of documents: %d' % len(corpus_adv))\n",
    "\n",
    "print('#Number of unique tokens: %d' % len(dictionary_dadv))\n",
    "print('#Number of documents: %d' % len(corpus_dadv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('corpus_adv.pkl', 'wb') as lf:\n",
    "#     pickle.dump(corpus_adv, lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('corpus_dadv.pkl', 'wb') as lf:\n",
    "#     pickle.dump(corpus_dadv, lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics_adv = 6\n",
    "passes = 5\n",
    "model_adv = LdaModel(corpus = corpus_adv, id2word = dictionary_adv,passes = passes, num_topics = num_topics_adv,random_state = 7)\n",
    "model_adv.save(\"tp_adv_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_dadv = 7\n",
    "model_dadv = LdaModel(corpus = corpus_dadv, id2word = dictionary_dadv,passes = passes, num_topics = num_topics_dadv,random_state = 7)\n",
    "model_dadv.save(\"tp_dadv_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "[(0, '0.011*\"안정\" + 0.009*\"공공기관\" + 0.009*\"서울\" + 0.008*\"부바\" + 0.007*\"유연근무제\" + 0.007*\"육아휴직\" + 0.006*\"정년\" + 0.006*\"강도\" + 0.006*\"높은\" + 0.005*\"공기업\"'), (1, '0.013*\"점심\" + 0.009*\"저녁\" + 0.009*\"버스\" + 0.009*\"기숙사\" + 0.008*\"통근\" + 0.008*\"높은\" + 0.008*\"식당\" + 0.007*\"맛있음\" + 0.006*\"수당\" + 0.006*\"아침\"'), (2, '0.013*\"데이\" + 0.013*\"금요일\" + 0.012*\"높은\" + 0.008*\"포인트\" + 0.008*\"제도\" + 0.008*\"리프\" + 0.007*\"패밀리\" + 0.007*\"업계\" + 0.006*\"대비\" + 0.006*\"여름\"'), (3, '0.013*\"재택근무\" + 0.008*\"재택\" + 0.008*\"대기업\" + 0.007*\"동료\" + 0.007*\"포인트\" + 0.007*\"기회\" + 0.006*\"업계\" + 0.006*\"사내\" + 0.005*\"개인\" + 0.005*\"교육\"'), (4, '0.008*\"수당\" + 0.006*\"강도\" + 0.006*\"대기업\" + 0.005*\"따라\" + 0.005*\"명절\" + 0.005*\"때문\" + 0.005*\"나옴\" + 0.005*\"지급\" + 0.005*\"사업\" + 0.005*\"없고\"'), (5, '0.012*\"카페\" + 0.012*\"커피\" + 0.011*\"건물\" + 0.010*\"점심\" + 0.009*\"간식\" + 0.009*\"할인\" + 0.009*\"사내\" + 0.009*\"식대\" + 0.008*\"사옥\" + 0.007*\"사무실\"')]\n",
      "\n",
      " DADV\n",
      "[(0, '0.010*\"생산\" + 0.009*\"진급\" + 0.006*\"군대\" + 0.005*\"사원\" + 0.005*\"공장\" + 0.005*\"수직\" + 0.005*\"보수\" + 0.005*\"출근\" + 0.005*\"꼰대\" + 0.005*\"승진\"'), (1, '0.026*\"계약\" + 0.015*\"정규직\" + 0.008*\"현장\" + 0.007*\"강도\" + 0.006*\"사업\" + 0.005*\"전환\" + 0.005*\"수직\" + 0.005*\"교대\" + 0.005*\"군대\" + 0.005*\"차이\"'), (2, '0.007*\"프로젝트\" + 0.006*\"무량\" + 0.006*\"평가\" + 0.005*\"조직\" + 0.004*\"소통\" + 0.004*\"리더\" + 0.004*\"팀바팀\" + 0.004*\"부분\" + 0.004*\"좋은\" + 0.004*\"성과\"'), (3, '0.012*\"위치\" + 0.006*\"행정\" + 0.006*\"연구원\" + 0.006*\"출퇴근\" + 0.005*\"수당\" + 0.005*\"식당\" + 0.005*\"인상\" + 0.005*\"교통\" + 0.005*\"연구\" + 0.005*\"주변\"'), (4, '0.009*\"임원\" + 0.007*\"사업\" + 0.007*\"영진\" + 0.006*\"영업\" + 0.005*\"경영\" + 0.005*\"미래\" + 0.005*\"매출\" + 0.005*\"조직\" + 0.004*\"성장\" + 0.004*\"정치\"'), (5, '0.011*\"고객\" + 0.009*\"승진\" + 0.008*\"영업\" + 0.007*\"민원\" + 0.007*\"본사\" + 0.007*\"실적\" + 0.007*\"스트레스\" + 0.007*\"순환\" + 0.006*\"매장\" + 0.006*\"압박\"'), (6, '0.007*\"수당\" + 0.006*\"출근\" + 0.006*\"사용\" + 0.005*\"대표\" + 0.005*\"관리자\" + 0.004*\"교육\" + 0.004*\"관리\" + 0.004*\"진짜\" + 0.004*\"주말\" + 0.004*\"사원\"')]\n"
     ]
    }
   ],
   "source": [
    "print('ADV')\n",
    "print(model_adv.print_topics(num_words=10))\n",
    "print('\\n DADV')\n",
    "print(model_dadv.print_topics(num_words=10))\n",
    "# print('#topic distribution of the first document:',model.get_document_topics(corpus)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_korean_words(input_string):\n",
    "    # Regular expression to match Korean words\n",
    "    korean_pattern = re.compile(\"[가-힣]+\")\n",
    "\n",
    "    # Find all matches in the input string\n",
    "    korean_matches = korean_pattern.findall(input_string)\n",
    "\n",
    "    return korean_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "# list of important words from \n",
    "adv_topics = model_adv.print_topics(num_words=50)\n",
    "dadv_topics = model_dadv.print_topics(num_words=50)\n",
    "adv_words = []\n",
    "dadv_words = []\n",
    "\n",
    "for i in range (len(adv_topics)):\n",
    "    for j in range (len(adv_topics[i])):\n",
    "        if j%2 == 1:\n",
    "            adv_words += extract_korean_words(adv_topics[i][j])\n",
    "\n",
    "for i in range (len(dadv_topics)):\n",
    "    for j in range (len(dadv_topics[i])):\n",
    "        if j%2 == 1:\n",
    "            dadv_words += extract_korean_words(dadv_topics[i][j])\n",
    "\n",
    "adv_words = list(set(adv_words))\n",
    "dadv_words = list(set(dadv_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_to_pickle(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "# Example usage\n",
    "# Specify the filename\n",
    "adv_filename = \"adv_word_list.pkl\"\n",
    "dadv_filename = \"dadv_word_list.pkl\"\n",
    "\n",
    "# Save the list to a pickle file\n",
    "save_to_pickle(adv_words, adv_filename)\n",
    "save_to_pickle(dadv_words, dadv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "textmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
