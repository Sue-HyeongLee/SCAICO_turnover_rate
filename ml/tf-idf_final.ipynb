{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZZOglyMPdYr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1O1kT8AywCZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from matplotlib import rc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vlcga-TMC1J"
      },
      "outputs": [],
      "source": [
        "def get_mae(model):\n",
        "    train_pred = model.predict(X_train)\n",
        "    score = mean_absolute_error(y_train,train_pred)\n",
        "    print(model.__class__.__name__, \" train MAE: \", np.round(score,3))\n",
        "    pred = model.predict(X_test)\n",
        "    score = mean_absolute_error(y_test,pred)\n",
        "    print(model.__class__.__name__, \" MAE: \", np.round(score,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e52wUy3uMi0g"
      },
      "outputs": [],
      "source": [
        "def get_maes(models):\n",
        "    scores = []\n",
        "    for model in models:\n",
        "        score = get_mae(model)\n",
        "        # scores.append(score)\n",
        "    # return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHg414Gbss6n"
      },
      "outputs": [],
      "source": [
        "def get_rmse(model):\n",
        "    train_pred = model.predict(X_train)\n",
        "    train_mse = mean_squared_error(y_train,train_pred)\n",
        "    rmse = np.sqrt(train_mse)\n",
        "    print(model.__class__.__name__, 'train RMSE: ', np.round(rmse,3))\n",
        "    pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(model.__class__.__name__, ' RMSE: ', np.round(rmse,3))\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw11k7BKst6e"
      },
      "outputs": [],
      "source": [
        "def get_rmses(models):\n",
        "    rmses = []\n",
        "    for model in models:\n",
        "        rmse = get_rmse(model)\n",
        "        # rmses.append(rmse)\n",
        "    # return rmses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A-UePW7TjYZ"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for turn over rate\n",
        "def tor_preprocessing(tor):\n",
        "  tor_df = tor\n",
        "  tor_val = tor_df['turn_over_rate']\n",
        "\n",
        "  for i, val in enumerate(tor_val):\n",
        "    if '정보' in val:\n",
        "      tor_df.loc[i,'turn_over_rate'] = np.NAN\n",
        "    elif val == '(9999%)' :\n",
        "      tor_df.loc[i,'turn_over_rate'] = np.NAN\n",
        "    elif '(' in val:\n",
        "      tmp = float(val[1:-1].replace('%',''))*0.01\n",
        "      tor_df.loc[i,'turn_over_rate'] = tmp\n",
        "\n",
        "  return tor_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEHVA8MtTjeu"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for financial variable\n",
        "def fv_preprocessing(fv):\n",
        "  fv_df = fv\n",
        "  afv = fv_df['average_salary']\n",
        "  tfv = fv_df['total_sale']\n",
        "\n",
        "  for i, val in enumerate(afv):\n",
        "    if type(val) == str:\n",
        "        if '만원' in val:\n",
        "          tmp = int(val[:-2].replace(',',''))\n",
        "          fv_df.loc[i,'average_salary'] = tmp\n",
        "        if '회사' in val:\n",
        "          fv_df.loc[i,'average_salary'] = np.NAN\n",
        "        if '수집' in val:\n",
        "          fv_df.loc[i,'average_salary'] = np.NAN\n",
        "\n",
        "  for i, val in enumerate(tfv):\n",
        "    if type(val) == str:\n",
        "        if '조원' in val:\n",
        "          tmp = float(val[:-2].replace(',',''))*1000\n",
        "          fv_df.loc[i,'total_sale'] = tmp\n",
        "        if '억원' in val:\n",
        "          tmp = float(val[:-2].replace(',',''))\n",
        "          fv_df.loc[i,'total_sale'] = tmp\n",
        "        if '억' in val:\n",
        "          tmp = float(val[:-1].replace(',',''))\n",
        "          fv_df.loc[i,'total_sale'] = tmp\n",
        "          print(fv_df.loc[i,'total_sale'])\n",
        "          print(i)\n",
        "        if '회사' in val:\n",
        "          fv_df.loc[i,'total_sale'] = np.NAN\n",
        "\n",
        "  return fv_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf_2hCA7PjDe"
      },
      "outputs": [],
      "source": [
        "combined_data = pd.read_csv(\"bow_df_tf-idf.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXS7sVi9PtxX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "combined_data = combined_data[combined_data['turn_over_rate']<1]\n",
        "error_firm = ['동원홈푸드','휠라홀딩스','트리','와디즈','키위컴퍼니','줌인터넷','시선인터내셔널','브이티코스메틱','유니슨이테크','씨엠비대전방송','서울비젼','더메인즈','조은시스템']\n",
        "for i in error_firm:\n",
        "    combined_data = combined_data[combined_data.company_name != i]\n",
        "combined_data.dropna(axis=0, inplace=True)\n",
        "combined_data = combined_data.drop(['Unnamed: 0'], axis=1)\n",
        "combined_data = fv_preprocessing(combined_data)\n",
        "tor = combined_data['turn_over_rate']\n",
        "combined_data.drop('turn_over_rate', axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iO6ytOazvEK"
      },
      "outputs": [],
      "source": [
        "combined_data = combined_data.drop(['company_name'], axis = 1, inplace = False)\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(combined_data, tor, test_size=0.2)\n",
        "\n",
        "train_input['adv_dadv_combined'] = train_input['adv'] + ' ' + train_input['dadv']\n",
        "test_input['adv_dadv_combined'] = test_input['adv'] + ' ' + test_input['dadv']\n",
        "\n",
        "train_text = train_input['adv_dadv_combined'].to_numpy()\n",
        "test_text = test_input['adv_dadv_combined'].to_numpy()\n",
        "\n",
        "financial_train1=train_input['average_salary'].to_numpy().reshape(-1, 1).astype(float)\n",
        "financial_train2=train_input['total_sale'].to_numpy().reshape(-1, 1).astype(float)\n",
        "financial_test1=test_input['average_salary'].to_numpy().reshape(-1, 1).astype(float)\n",
        "financial_test2=test_input['total_sale'].to_numpy().reshape(-1, 1).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFW9MeovVu70"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidfvector = TfidfVectorizer(max_features=10000)\n",
        "text_train = tfidfvector.fit_transform(train_text)\n",
        "text_test = tfidfvector.transform(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY_wCXWvZ4ls"
      },
      "outputs": [],
      "source": [
        "# financial_train1 = financial_train1.reshape(-1, 1).astype(float)\n",
        "# financial_train2 = financial_train2.reshape(-1, 1).astype(float)\n",
        "# financial_test1 = financial_test1.reshape(-1, 1).astype(float)\n",
        "# financial_test2 = financial_test2.reshape(-1, 1).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEU7WUnGaRyn"
      },
      "outputs": [],
      "source": [
        "# financial_train1 = financial_train1.astype(float)\n",
        "# financial_train2 = financial_train2.astype(float)\n",
        "# financial_test1 = financial_test1.astype(float)\n",
        "# financial_test2 = financial_test2.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kLsPxaYXicc"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack # tf-idf로 변환된 텍스트 데이터와 금융 데이터를 연결\n",
        "X_train = hstack([text_train, financial_train1, financial_train2])  # 텍스트 데이터와 금융 데이터를 수평으로 연결\n",
        "X_test = hstack([text_test, financial_test1, financial_test2])    # 테스트 데이터에 대해서도 동일하게 수행\n",
        "y_train = train_target.to_numpy()\n",
        "y_test = test_target.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUAYgbyFZT5a",
        "outputId": "18175804-2306-4c5e-d6c0-f114c89cecfe"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "\n",
        "lr_reg = LinearRegression()\n",
        "lr_reg.fit(X_train, y_train)\n",
        "\n",
        "ridge_reg = Ridge()\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "\n",
        "lasso_reg = Lasso()\n",
        "lasso_reg.fit(X_train,y_train)\n",
        "\n",
        "models = [lr_reg, ridge_reg, lasso_reg]\n",
        "get_rmses(models)\n",
        "print('\\n')\n",
        "get_maes(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYmK5Ly6INZg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def print_best_params(model, params):\n",
        "\n",
        "    grid_model = GridSearchCV(model, param_grid=params, verbose=3,\n",
        "                              scoring='neg_mean_squared_error', cv=5)\n",
        "    grid_model.fit(X_train.astype(float),y_train.astype(float))\n",
        "    mse = -1 * grid_model.best_score_\n",
        "    rmse = np.sqrt(-1*grid_model.best_score_)\n",
        "    print('train: {0} After 5 CV, best average MSE: {1}, best average RMSE: {2}, best: {3}'.format(model.__class__.__name__,np.round(mse,4),\n",
        "                                                                           np.round(rmse,4),grid_model.best_params_))\n",
        "\n",
        "    grid_score = grid_model.score(X_test.astype(float), y_test.astype(float))\n",
        "    mse = -1 * grid_score\n",
        "    rmse = np.sqrt(-1*grid_score)\n",
        "    print('test: {0}  MSE: {1}, RMSE: {2}, best alpha: {3}'.format(model.__class__.__name__,np.round(mse,4),\n",
        "                                                                           np.round(rmse,4),grid_model.best_params_))\n",
        "    return grid_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW3UcrAPTjUk"
      },
      "outputs": [],
      "source": [
        "def get_MAE_RMSE(model):\n",
        "  # 모델을 사용하여 예측값 생성\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # MAE 계산\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "  # rmse\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  rmse = np.sqrt(-mse)\n",
        "  print('MSE: {0}, RMSE: {1}, MAE: {2}'.format(np.round(mse,4),np.round(rmse,4),np.round(mae,4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GOjn0CqRWAN_"
      },
      "outputs": [],
      "source": [
        "def graph_regessor_model(model):\n",
        "    font_path = \"/System/Library/Fonts/Supplemental/Arial Unicode.ttf\"\n",
        "    rc('font', family='AppleGothic')\n",
        "    # 특성 중요도를 가져옵니다.\n",
        "    importances = model.feature_importances_\n",
        "    print(importances.shape)\n",
        "    # 중요도가 높은 순으로 정렬된 인덱스를 가져옵니다.\n",
        "    sorted_indices = importances.argsort()[::-1]\n",
        "    # 상위 N개의 단어와 중요도를 가져옵니다.\n",
        "    # 중요도가 높은 순으로 정렬된 인덱스 추출\n",
        "\n",
        "    top_n = 12\n",
        "    top_indices = sorted_indices[:top_n]\n",
        "\n",
        "    # 상위 N개 단어와 가중치 추출\n",
        "    top_words = [tfidfvector.get_feature_names_out()[index] for index in top_indices if index < len(tfidfvector.get_feature_names_out())]\n",
        "    top_importances = [importances[index] for index in top_indices if index < len(tfidfvector.get_feature_names_out())]\n",
        "\n",
        "    # 그래프로 시각화합니다.\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(top_words, top_importances, color='skyblue')\n",
        "    plt.xlabel('중요도')\n",
        "    plt.title('상위 12개 단어의 중요도 (RF)')\n",
        "    plt.gca().invert_yaxis()  # 중요도가 높은 순서대로 표시하기 위해 Y 축 뒤집기\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor \n",
        "\n",
        "xgb_params = {'n_estimators': [500, 750, 1000], \n",
        "              'learning_rate': [0.05, 0.1, 0.15, 0.2], \n",
        "              'max_depth' : [4,6,8,10]}\n",
        "xgb_reg = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, colsample_bytree = 0.5, subsample = 0.8)\n",
        "print_best_params(xgb_reg, xgb_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_reg = XGBRegressor(n_estimators = 500, learning_rate = 0.05, max_depth = 4, colsample_bytree = 0.5, subsample = 0.8)\n",
        "xgb_reg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_MAE_RMSE(xgb_reg)\n",
        "graph_regessor_model(xgb_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Light GBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "lgbm_params = {'n_estimators': [500, 750, 1000], \n",
        "              'learning_rate': [0.05, 0.1, 0.15, 0.2], \n",
        "              'num_leaves' : [4, 6, 8, 10]}\n",
        "lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate= 0.05, num_leaves=4,\n",
        "                         subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)\n",
        "\n",
        "print_best_params(lgbm_reg, lgbm_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate= 0.05, num_leaves=4,\n",
        "                         subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)\n",
        "lgbm_reg.fit(X_train,y_train)                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_MAE_RMSE(lgbm_reg)\n",
        "graph_regessor_model(lgbm_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_params = {'n_estimators': [500, 750, 1000], \n",
        "              'learning_rate': [0.05, 0.1, 0.15, 0.2], \n",
        "              'max_depth' : [4,6,8,10]}\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=1000, learning_rate= 0.05, subsample=0.6 )\n",
        "\n",
        "print_best_params(gb_reg,gb_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gb_reg = GradientBoostingRegressor(n_estimators=1000, learning_rate= 0.05, subsample=0.6 )\n",
        "gb_reg.fit(X_train,y_train)                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_MAE_RMSE(gb_reg)\n",
        "graph_regessor_model(gb_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ss0jhNkaV3C3"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_params = {'n_estimators': [500, 750, 1000],\n",
        "             'max_depth' : [4,8,10]}\n",
        "rf_reg = RandomForestRegressor(n_estimators=1000)\n",
        "print_best_params(rf_reg,rf_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Voiyj_4lV8qs"
      },
      "outputs": [],
      "source": [
        "rf_reg = RandomForestRegressor(n_estimators=1000, max_depth= 10)\n",
        "rf_reg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_regessor_model(rf_reg)\n",
        "get_MAE_RMSE(rf_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "74nE5W6PWBbu"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "dt_params = {'max_depth' : [4,6,8,10]}\n",
        "dt_reg = DecisionTreeRegressor(max_depth=4)\n",
        "print_best_params(dt_reg, dt_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_reg = RandomForestRegressor(max_depth= 4)\n",
        "dt_reg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r1W2WsZJWSEZ"
      },
      "outputs": [],
      "source": [
        "get_MAE_RMSE(dt_reg)\n",
        "graph_regessor_model(dt_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8jPBBPlsmpLb"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(xgb_reg, 'tf_idf_xgb_reg_model.pkl')\n",
        "joblib.dump(lgbm_reg, 'tf_idf_lgbm_reg_model.pkl')\n",
        "joblib.dump(gb_reg, 'tf_idf_gb_reg_model.pkl')\n",
        "joblib.dump(rf_reg, 'tf_idf_rf_reg_model.pkl')\n",
        "joblib.dump(dt_reg, 'tf_idf_dt_reg_model.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "textmining",
      "language": "python",
      "name": "textmining"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
